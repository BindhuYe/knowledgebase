- We’ll start with one of the simplest approaches by using the built-in kubectl proxy command. It’s mainly used for debugging and local development if you need to get access to the Kubernetes API server. The kubectl proxy command creates a proxy server or application-level gateway between localhost and the Kubernetes API server. For example, you can access a deployed Nginx pod in Kubernetes by navigating to the following URL. The main difference between the kubectl proxy and the kubectl port-forward command is that

- the proxy only works on the application level and can only forward HTTP and HTTPS requests from Kubernetes to localhost. You can find the source in my GitHub repository; the link will be in the video description. If you want to reproduce one of the examples, you can use my Terraform code to create an EKS cluster and apply the corresponding Kubernetes manifests for each example. Now, I'll demonstrate how to make your Nginx deployment accessible from your local computer using the kubectl proxy command.

- First and foremost, we need a service. Please note that if you have a specifically named port, like 'web', the URL you'll use to access this service will be a bit different. Let's get the kubectl proxy up and running, setting it to use port 8080 on our local computer. As I mentioned earlier, not only can you access the applications running in Kubernetes, but you can also directly access the Kubernetes API itself. For instance, you can view all the pods currently running in Kubernetes by sending a ‘HTTP get' request to 'api/v1/pods'.

- Also, you can sort through the Kubernetes objects using namespaces. As an example, let's see how we can find the Nginx service that we just set up. To access the application itself, you'd follow a specific format. First is the protocol, which can be either http or https. Then comes the service name, which in this case is Nginx, followed by the port name. If you don't have a named port, just leave it blank like this. Lastly, we need to specify that we want to route this application to our local computer.

- However, this method isn't very flexible—it can only forward static content and reveal the API if you want to experiment with it. One of the most frequent uses of the kubectl proxy command is to pair it with the Kubernetes dashboard. If you've applied the contents of the 'Example 1' folder, it will deploy the dashboard and create a user with admin rights to access this dashboard. The only remaining step is to create a temporary token by running the following command. Now, take this token and go to the Kubernetes-dashboard URL.

- You'll see that this service uses https and doesn't have a named port. Now, let's go ahead and press enter. Once the dashboard loads, we need to insert our token. Now you can access all objects in Kubernetes from this dashboard. While it's a good starting point for monitoring Kubernetes, it might not be sufficient for production use. I recommend using Prometheus to monitor your applications and infrastructure. Another similar approach that you can use for debugging and collecting some metrics from your applications is to use the kubectl port-forward command.

- It directly binds a port from within the Kubernetes application to your localhost. Compared to kubectl proxy, port-forward works on the 4th layer of the OSI model, which allows it to forward arbitrary protocols based on TCP. Keep in mind that UDP is not yet supported. For example, you can port-forward a custom MongoDB Wire Protocol to the localhost. Also, I'll show you how to use this command and set up custom domains locally; for instance, instead of using localhost, you can use 'mongo' instead.

- If you apply the 'example 2' folder, you'll get two pods. One is for mongodb and the other is for the 'service-a' app based on the Nginx image. Both of these applications are deployed as deployment objects. Also, both of them have a service of the ClusterIP type. Now, the simplest way to port forward application is to use a pod ID, specify the namespace in which it's running, and provide the local and remote port on the pod itself.

- By default, Mongo uses this port number. You can also specify the type of Kubernetes object you want to port-forward. For example, this command is exactly the same as the previous one. Now, sometimes you don't want to copy and paste the pod ID. Instead, you can use the deployment name. In this case, it will forward traffic to one of the pods under that deployment object. Also, instead of using a deployment, you can use a service. In this case, we can port-forward the MongoDB service to the localhost.

- This will only bind to a single pod under that service. If your pod or service exposes multiple ports - for example, one port for your application and another for metrics - you can forward them simultaneously by providing additional port mapping. If you want to use the same port number that your pod is using, a shortcut is to just specify a single port. This will then map it to your localhost. Let's give this a shot with the mongo shell, which you can install using the brew package manager on a Mac, for example.

- Let's run a ping to check the connection. Great, it works! Another handy trick is to use a colon; in this case, kubectl will bind to any available port on the localhost. This can sometimes be useful to avoid conflicts. Now, if you, like me, use port-forward a lot, you might prefer to use exactly the same DNS as you would when accessing an app inside Kubernetes. For this, we can use the kubefwd tool. It can port forward all the services in a given namespace to the localhost while keeping the DNS the same.

- Let's test this out. Use kubefwd to expose all services in the 'example-2' namespace. You'll also need to use 'sudo' to grant it access to your '/etc/hosts' file, so it can create custom local DNS names. Additionally, if you're using ports lower than 1024, you'll need 'sudo' for that as well. Now, let's try to use the same mongo shell with the mongo host. If you execute the ping command to verify the connection, it should work. This is the last method solely used for debugging. From now on, we'll expose apps that anyone on the internet can access.

- Typically, you would provision all your EKS nodes in the private subnets and use other methods to expose apps to the internet. But let's say you created one EKS node group in a public subnet. Each Kubernetes node will get a primary IP address derived from the subnet CIDR, as well as a public IP address. By the way, you need to enable 'map_public_ip_on_launch' for that. Now, when you deploy your applications to EKS, each pod will get a secondary IP address. Depending on the instance family, different nodes can have a different number of allocatable secondary IP addresses.

- This can limit the number of pods you can run on a single node. The same exact process also applies to private nodes. For instance, if you deploy a service-b, it might get a secondary IP address like 10.0.68.11, which is separate from the public IP. Now, if you want to expose your app to the internet or maybe just within your VPC, you could decide to use 'hostNetwork'. This option is rarely used and is usually reserved for very specific use cases. Nonetheless, when you enable 'hostNetwork' on the pod, the pod will have access to all network interfaces on that node and be able to attach itself to the primary IP address.

- On AWS, the public IP address is translated to a private one, so the pod doesn't directly see the public IP address. I wouldn't recommend this approach for production use for many different reasons. With 'hostNetwork', you must access the exact node where the pod is deployed. For instance, when using the 'nodePort' (which we'll talk about later), you can access any node - even if the pod isn't running there - and the request will be directed to the

- correct node. Now, just in case you still decide to use 'hostNetwork' to expose the application, I would suggest using 'daemonset' and utilizing affinity to deploy them on specific nodes. In that scenario, all your nodes would host that service and you could access any of them. Alright, let’s put this into practice. First of all, you need to open a port on the nodes to handle requests. EKS automatically creates a security group, and we can use it to open port 80 on all nodes. Also, we have an EKS node group set up in the public subnet, labeled with a role equal to 'public nodes'.

- We'll use this label to select these nodes from the 'daemonset' later. On the other hand, in the 'daemonset', we have a 'nodeAffinity' set to deploy our application only to these public nodes. Firstly, let's deploy it without 'hostNetwork' to observe the difference. Once you've deployed it, and since we have only one public node, we'll see a single pod running. Also, make a note of the IP address that this pod uses. If you open the AWS console and locate that EC2 instance, you'll notice the pod's IP listed

- under the secondary private IP addresses. As I mentioned earlier, each pod will, by default, be assigned a secondary private IP. Now let's enable 'hostNetwork' on the 'daemonset', by setting this parameter to true. If you redeploy the 'daemonset' now and get the IP address using the same 'kubectl get pods' command, you'll notice in the AWS console that the pod is now bound to the primary IP addresses. This means it has access to the public IP address as well. Since we've already opened the firewall, we can use 'curl' to access this public IP address from anywhere in the world.

- Typically, you would provide a DNS name instead of an IP address to your clients. I already have a public hosted zone in AWS, so let me create a DNS record for 'service-b', pointing to the node's public IP address. Now we can access our service using the DNS name. The biggest issue with this approach is that you constantly need to update your DNS name with the correct IP addresses. For example, if you add more nodes, you need to manually add all the IP addresses here.

- Also, all these public IP addresses, as well as the private EC2 instance IP addresses, are temporary. If you stop or terminate your instance, you'll lose this IP, and you'll need to update it in the DNS, which could lead to service disruption for your clients. You can be sure that Kubernetes nodes come and go all the time, and you should never rely on these nodes or static IP addresses. This approach could work maybe for debugging or proof of concept, so please don't use it in production unless you know what you're doing.

- Another way you can make your application accessible from Kubernetes is by using a NodePort service. It exposes the service on each Node’s IP addresses at a static port. By default, the Kubernetes control plane allocates a port from a specific range. This can also be modified with the --service-node-port-range flag. Then, each node proxies that port (the same port number on every Node) into your Service. Additionally, every node in the cluster configures itself to listen on that assigned port and to forward traffic to the node where your application is running.

- The NodePort is commonly used with cloud load balancers like nlb or alb. This is set up for you automatically by EKS since it needs to assign the port and manage firewalls. Additionally, using a NodePort lets you create your own load balancing solution and set up environments that Kubernetes doesn't completely support. Alternatively, you can use NodePort directly. This might be useful if you're looking to save money by not using a load balancer, or if you need a quick proof-of-concept for a client.

 -This approach is similar to using hostNetwork. If you give your clients a DNS, you'll need to continually update the DNS with all the Kubernetes IP addresses. Therefore, this direct approach is rarely used in real-life production environments. Regardless, let me give you a quick demo to show you how it works. Alright, in this fourth example, we have a typical nginx deployment object. Now, to create a NodePort, you need to change the type of the service, and optionally specify a port within the range we discussed before.

- If you leave this option out, Kubernetes will automatically assign a random port. However, this could make managing security groups more challenging, which is why I've chosen to assign a fixed port number, 30010. Once you apply the changes, you'll end up with a NodePort type service. It's crucial to understand that when you manually create this NodePort service, you also need to open a firewall. You can visit the AWS console to fetch the new public IP addresses for the node.

- However, if you attempt to access this port now, using a tool like netcat for instance, you'll face a timeout. Now, let's make another security group rule to open this 30010 port to all IP addresses. We can also use the EKS resource attribute to gain access to the EKS security group. Once you've applied the changes using Terraform, you should be able to access this port. Also, if you open your web browser and navigate to the appropriate address, you should see the default nginx HTML page. This page is being served by the pod running in the EKS cluster.

- The drawback of this method is that when you create DNS records, you have to manually manage IP addresses. Let's proceed and create an A record for nginx pointing to its IP. Also, unlike with hostNetwork, you can use any public IP from any node in the public subnet. Once you apply these changes and wait a few minutes for DNS to be provisioned, you should be able to access your service in Kubernetes using this DNS name. From this point forward, we'll shift to more robust, production-ready methods for exposing services from Kubernetes.

- The next step we can take to make our applications accessible is by using a 'LoadBalancer' type of service. It's important to remember that each Kubernetes comes with inbuilt cloud manager controllers that handle the managment of some basic cloud elements. For instance, the AWS built-in controller can create load balancers and establish both target groups and security group rules. This controller acts a lot like the NodePort we talked about before. To begin with, when you create a 'LoadBalancer' type of service, EKS will allocate a port on every node.

- By default, it creates what's called a 'classic load balancer', but it's generally best to avoid using these because there are newer and better solutions available, like network load balancers. I'll explain how to set it up using annotations a bit later. Next, the built-in controller creates a target group in AWS and directs all the traffic to those node ports. Then, just like with the NodePort type, Kubernetes will route the traffic to the service. Okay, let me show you how it works.

- We'll be using the exact same deployment that we used in our previous example. For the service, we'll switch it to the LoadBalancer type. Now, here's where you can use annotations to adjust how your load balancer operates. I strongly suggest changing it from the classic load balancer to a network load balancer. Optionally, you can create a private load balancer. This means the load balancer will only have private IP addresses and can only be accessed within your Virtual Private Cloud (VPC). There are other annotations you can use to fine-tune it.

- After you apply example 5, in a few seconds you should see an External IP, which is the network load balancer's DNS name. If it's in a pending state, you can usually describe the service to get an error message. You'll use this DNS name to create a CNAME record for your service. You can confirm that the load balancer is internet-facing and that it's a network load balancer in the AWS console. Next, click on 'listeners' and open the target group that was created by the controller. You'll see that we have two Kubernetes nodes registered as targets with a node port, which is basically the same as the NodePort type.

- Also, if you check the security group, which is managed by EKS, you'll notice that EKS added an extra rule to make this node port accessible from the internet. This is because network load balancers, unlike application load balancers, don't have their own security groups. They inherit rules from the underlying EC2 instances. Now, wait until the load balancer is fully provisioned, which may take a few minutes, and then you'll be able to access it through a web browser.

- Managing DNS is simpler with a load balancer. All you need to do is change the 'A' record to a 'CNAME' record and add the load balancer's DNS name as a value. Once you've done that, you should be able to access the Nginx custom domain, and it will directly route you to your pod inside Kubernetes. Recently, AWS developed its own cloud manager controller and made it open-source. It's called the AWS Load Balancer Controller. This controller replaces the built-in or 'in-tree' controller that's part of Kubernetes.

- With this new controller, you can create both 'ingresses' (which we'll discuss later) and load balancer services. You also need to manually install it in your EKS cluster and provide access to the AWS API using the OpenID Connect provider. The biggest difference between this controller and the built-in one is that it can route traffic directly to the pod, because the pod uses and gets a VPC-native IP address. This is called secondary IP addresses, which I've covered in previous examples

- This eliminates the network hop. Now, the request doesn't need to first hit the node IP address and then be routed to the pod IP. It can directly create an AWS target group and point it to the pod IP address, which should also reduce latency for your requests. When the pod gets rescheduled, the load balancer controller will automatically update the load balancer target IP address for this pod. Now, it's time for a demo. We have the same Nginx deployment object again. In the service, you also use the same LoadBalancer type, but you must set the load balancer type annotation to 'external', otherwise it will be managed by the built-in controller.

- This controller supports both target types; if you set 'instance' here, you'll get the same NodePort type target group. Instead, you want to set it to 'IP' mode to directly route traffic to the pod. Also, the network load balancer is the default type for this controller and you can use other annotations to configure your load balancer. You can also enable the proxy v2 protocol and other things. To deploy the AWS Load Balancer Controller, first you need to create an OpenID Connect provider.

- Then, you need to create an IAM role with the necessary permissions so that this controller can manage load balancers on your behalf. Lastly, we can use Helm to deploy this controller to the EKS cluster. The key thing here is that you need to add a service account annotation to bind the Kubernetes RBAC (Role-Based Access Control) system with the AWS IAM role. First, let's get the pod's IP address. Keep a note of this address; it should be in the target group in AWS. Then, check the service to make sure that the load balancer was created.

- Again, if it's pending, describe the service to figure out why it's failing. Next, let's locate this load balancer in the AWS console. As you can see, it's a network type and it's internet-facing as well. Next, let's open the target group associated with this load balancer. Here, under 'targets', you should see the pod's IP address and the direct service port number, which in our case is port 80. This confirms that this controller uses native network routing and doesn't rely on NodePorts anymore.

- Let's use the load balancer's DNS name to check that we can access the default Nginx HTML page. If you want to create a DNS record, similar to the previous example, you just need to create a single CNAME record pointing to the load balancer's DNS name. The next option that you have is to use 3rd party ingress controllers such as Nginx ingress, Kong ingress, and many others. You can find a list of some ingress controllers on the official Kubernetes website. Typically, you would use ingress controllers to route HTTP or HTTPS traffic to your services.

- But actually, many ingresses also support routing custom TCP and UDP protocols. I have a tutorial on my channel if you want to learn more about Nginx ingress. Now, first, you must deploy an ingress controller to your EKS cluster just like any other application. The ingress controller will create a service of type load balancer, which you'll use to route requests to your service. Then, the ingress controller, in this case, Nginx, will listen for any Ingress resources created in the cluster. If you specify that this ingress must be implemented with the Nginx ingress controller, it will configure it to route requests.

- It's important to understand that every single request from clients will go through the load balancer, which is almost always a layer 4 network load balancer. Then, those requests will be directed to the ingress controller itself. Based on the rules, the Nginx controller will route them to the correct service. It adds an additional network hop, but it also brings an opportunity to scrape metrics from a ingress pod with Prometheus to get traffic, latency, and errors for all ingresses without implementing anything on the application side. It's very useful, and I use it all the time.

- When you create additional ingresses, those ingresses get converted to Nginx native Lua configuration and routes to the target service. So, all your ingresses will use the same load balancer. This is the biggest selling point - you need a single load balancer, which can reduce the cost and maintenance. My favorite feature is the metrics that I can get from the ingress controller. 

- First of all, we need to deploy Nginx ingress to the EKS cluster. The simplest way is to use Helm charts, of course. By default, it uses the Nginx ingress class name. I usually have at least two different ingresses: one to expose services to the internet and another to expose services within the VPC. I suggest that you also set up the AWS load balancer controller and use it to create a network load balancer with VPC native routing using pod IPs. You'll need all three annotations for this. Looking at the YAML file, we have the usual Nginx deployment.

- Then there's a regular ClusterIP service to distribute traffic among the pods. Finally, we have the ingress object. This one uses simple DNS-based routing. There are many adjustments you can make here, and I have a separate tutorial with many examples if you want to learn more. After you apply the Terraform configuration, you should have an external-nginx ingress class. Additionally, if you followed my suggestion, you should have another one, an alb class, created by the AWS load balancer controller.

- Once you apply the files from the example 7 folder, you should see the load balancer DNS name in a few seconds. This should match the network load balancer created by the Nginx ingress Helm chart. Sometimes, it might take a few minutes for this address to appear. It especially takes a bit of time to fully provision the network load balancer, so be patient. When that's done, you can use curl to check if it works, even without setting up the DNS yet. You can just pass the host header to simulate the DNS on your ingress object.

- Next, let's create a CNAME record that points to that load balancer. Now, you should be able to access service-a in your web browser. The ingress should then direct that request to the service-a pod in Kubernetes. Another approch you could try is to use the AWS load balancer controller to create Ingresses. This approach has its own advantages and disadvantages. On the upside, you don't need run an extra proxy controller in Kubernetes. When you create an ingress, the load balancer controller will, by default, create a dedicated application load balancer in AWS, which operates at layer 7.

- It'll then direct traffic straight to the Pod IP addresses, assuming you choose to use IP mode. The architecture seems simpler, which I really like, but because there's no controller to proxy requests, you can't use Prometheus to monitor all your ingresses unless you export some metrics from AWS. Initially, the main reason for using Ingresses was to share a single load balancer across all services. Now, the load balancer controller will create a dedicated application load balancer for each ingress. There's a workaround to use groups and put your ingresses into one application load balancer, but then managing certificates and pulling metrics for your ingresses becomes almost impossible. So, most of the time, you end up using one load balancer per ingress. Also, this type of ingress doesn't support regex expressions and route-based routing very well. Application load balancers are more expensive and slower compared to network load balancers since they operate at layer 7 of the OSI model.

- So, do your own research before committing to this approach! Just remember, both types of ingress controllers, like Nginx and the native AWS load balancer controller, definitely have their own use cases. To use the AWS load balancer controller, you first need to deploy it using Helm, just like we did in the previous examples. This will create an alb ingress class that you can use for your ingress object. Now, for the ingress, we want to use IP mode to route traffic directly to the pod IP without node ports, and set it to internet facing. By default, it will create a private load balancer, and set the ingress class to alb. Once you apply it, you'll see a new load balancer get created. Let's check this out in the AWS console. You can see that it's now an application load balancer instead of a network load balancer. Under the listeners, you can find the rules where the ingress is managed. Now, if you decided to group all ingresses and use a single load balancer, all new ingresses would be converted to these rules.

- As you can see, it's going to be hard to manage certificates and pull metrics for each different ingress. Another compelling way to expose your application to the internet is to use Amazon API Gateway. It's especially useful if you use other AWS services. You can provide access from a single endpoint to multiple services in your AWS account. It can perform authorization and access control, throttling, monitoring, traffic management, and more. And the best part is that you can use different services such as serverless Lambda, DynamoDB, applications deployed on EC2 instances, and of course, you can expose the services from the EKS cluster. To expose an application through the API Gateway, you first need to expose it outside of Kubernetes using a private load balancer. Then, you would use a listener Amazon resource name and integrate it into the API Gateway. I've created a full guide if you want to dive deeper, but let's take a look at a quick demo for now. First, we need a deployment object. We're going to use an open-source tool called 'echoserver' that gives us metadata about the host and the request. Next, we need to make our application accessible using a private network load balancer. So, let's put that in place. Now we have the 'deployment' and the 'service', which is our load balancer. Next, head to the AWS console and get the 'arn' - the Amazon Resource Name. But be careful, we need the name of the 'listener', not the load balancer. This is really important, otherwise, it won't work. In the API gateway, we have to set up the gateway itself and the stage. We'll call our stage 'dev' for now.

- Then, we have to integrate the API gateway. At this point, you'll give the listener ARN and the path on the API gateway to send requests to the Kubernetes application. Once you've applied the terraform, you'll get a URL to use to access the service. But before we try that, let's get the 'pods' in this namespace. Take a note of the pod id, which is also the local DNS name of the pod. After a few minutes, once the gateway and load balancer are ready, you can use 'curl' to try to access it. You should see a response from the echoserver with the metadata and the server's host name. These are some of the most common approaches people use to make services available from the EKS cluster. 

